2024-10-16
- 前阵子刚学了这个算法，正好做实验有数据可以实践一下。报告里的拟合我用R语言弄的，都是别人写好的算法，这里我从头开始亲手拟合试一试。
- 本附录用的语言为python，环境为pycharm中的jupter-notebook插件，理论基础参见https://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf

#### 先把我之前用R拟合的代码列出来


```R
library(readxl)
library(openxlsx)
setwd("/Users/liqingyu/Desktop/Excel\ data")  # 设置工作目录
data <- read_excel("data1.xlsx")
df <- lm(y ~ x, data=data)
summary(df)
#plot(model)
plot(data$x, data$y, main="distance-time graph", xlab="distance/cm", ylab="time/ms", pch=19)

# 添加拟合直线
abline(df, col="red", lwd=2)
```

#### 下面进入正文


```python
import math,copy
import numpy as np
```

### 数据导入

其实就两行数据直接输入就行，这里导入Excel尝试一下


```python
import pandas as pd
df = pd.read_excel('/Users/liqingyu/Desktop/Excel data/data1.xlsx')
x = df['x']
y = df['y']
# 删去数据中的NaN值，用到了pandas
x,y = x.dropna(),y.dropna()
#将数据转化成数组
x = x.to_numpy()
y = y.to_numpy()
print(x)
print(y)
```

    [ 6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26.]
    [0.03933333 0.05333333 0.06583333 0.08083333 0.09666667 0.11
     0.12166667 0.13666667 0.15       0.16333333 0.17666667]


### 数学基础
先假设为线性模型，预测函数为$f_{w,b}(x^{(i)})$：
$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$
损失函数$J(w,b)$计算公式：
$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\tag{2}$$
传统的梯度下降：
$$\begin{align*} \text{ 重复至收敛:} \; \lbrace \newline
\;  w &= w -  \alpha \frac{\partial J(w,b)}{\partial w} \tag{3}  \; \newline 
 b &= b -  \alpha \frac{\partial J(w,b)}{\partial b}  \newline \rbrace
\end{align*}$$
参数 $w$, $b$ 需同时迭代。

梯度的具体计算：
$$
\begin{align}
\frac{\partial J(w,b)}{\partial w}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \tag{4}\\
  \frac{\partial J(w,b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{5}\\
\end{align}
$$

### 传统的梯度下降

#### 损失函数计算


```python
def compute_cost(x, y, w, b):
    # 获取数据容量
    m = x.shape[0] 
    cost = 0
    
    for i in range(m):
        f_wb = w * x[i] + b
        cost = cost + (f_wb - y[i])**2
    total_cost = 1 / (2 * m) * cost

    return total_cost
```

#### 梯度计算


```python
def compute_gradient(x, y, w, b):    
    m = x.shape[0]
    #初始化
    dj_dw = 0
    dj_db = 0
    
    #这里每计算一次都要遍历一遍数据，每次下降精准但耗时较长
    for i in range(m):  
        f_wb = w * x[i] + b 
        dj_dw_i = (f_wb - y[i]) * x[i] 
        dj_db_i = f_wb - y[i] 
        dj_db += dj_db_i
        dj_dw += dj_dw_i 
    dj_dw = dj_dw / m 
    dj_db = dj_db / m 
        
    return dj_dw, dj_db
```

#### 迭代函数


```python
def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): 
    # w = copy.deepcopy(w_in) # 避免修改全局w_in
    # 记录历史数据分析用
    J_history = []
    p_history = []
    b = b_in
    w = w_in
    
    for i in range(num_iters):
        dj_dw, dj_db = gradient_function(x, y, w , b)     

        # 同时迭代b和w
        b = b - alpha * dj_db                            
        w = w - alpha * dj_dw                            

        # 记录J
        if i<10000000:      # 差不多得了
            J_history.append( cost_function(x, y, w , b))
            p_history.append([w,b])
        # 间隔10%打印
        if i% math.ceil(num_iters/10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.6e}, b:{b: 0.6e}")
 
    return w, b, J_history, p_history 
```

#### 激动人心的拟合计算


```python
# 参数初始化
w_init = 0
b_init = 0
# 调试参数
iterations = 100000
tmp_alpha = 1.0e-6
# 梯度下降运行
w_final, b_final, J_hist, p_hist = gradient_descent(x ,y, w_init, b_init, tmp_alpha, 
                                                    iterations, compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent: ({w_final:8.8f},{b_final:8.8f})")
```

    Iteration    0: Cost 6.87e-03  dj_dw: -2.017e+00, dj_db: -1.087e-01   w:  2.017091e-06, b: 1.087273e-07
    Iteration 10000: Cost 2.08e-05  dj_dw: -1.036e-01, dj_db: -5.248e-03   w:  6.445964e-03, b: 3.450480e-04
    Iteration 20000: Cost 2.71e-06  dj_dw: -5.338e-03, dj_db:  6.471e-05   w:  6.777041e-03, b: 3.594161e-04
    Iteration 30000: Cost 2.66e-06  dj_dw: -2.921e-04, dj_db:  3.371e-04   w:  6.794222e-03, b: 3.568139e-04
    Iteration 40000: Cost 2.66e-06  dj_dw: -3.303e-05, dj_db:  3.506e-04   w:  6.795285e-03, b: 3.533448e-04
    Iteration 50000: Cost 2.66e-06  dj_dw: -1.970e-05, dj_db:  3.509e-04   w:  6.795520e-03, b: 3.498357e-04
    Iteration 60000: Cost 2.66e-06  dj_dw: -1.899e-05, dj_db:  3.504e-04   w:  6.795712e-03, b: 3.463290e-04
    Iteration 70000: Cost 2.66e-06  dj_dw: -1.893e-05, dj_db:  3.500e-04   w:  6.795901e-03, b: 3.428269e-04
    Iteration 80000: Cost 2.65e-06  dj_dw: -1.890e-05, dj_db:  3.495e-04   w:  6.796090e-03, b: 3.393296e-04
    Iteration 90000: Cost 2.65e-06  dj_dw: -1.888e-05, dj_db:  3.490e-04   w:  6.796279e-03, b: 3.358369e-04
    (w,b) found by gradient descent: (0.00679647,0.00033235)


#### 结果分析
- 一开始步长太大，cost函数发散，程序爆了；太小也不好，我定的迭代次数为100000次，测试发现1e-6最终损失值较小，1e-7损失反而大了需要增加迭代次数
- 斜率和R拟合的值比较接近（R为0.006890152），但是截距差挺多的，我又用卡西欧fx-991CN拟合截距约为-1.74e-03和R的结果相近，可能因为我的迭代次数还不够


```python
w_init = 0
b_init = 0
iterations = 10000000
tmp_alpha = 1.0e-5
w_final, b_final, J_hist, p_hist = gradient_descent(x, y, w_init, b_init, tmp_alpha,
                                                    iterations, compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent: ({w_final:8.8f},{b_final:8.8f})")
```

    Iteration    0: Cost 6.80e-03  dj_dw: -2.013e+00, dj_db: -1.086e-01   w:  2.012818e-05, b: 1.085758e-06
    Iteration 1000000: Cost 4.38e-07  dj_dw: -3.851e-06, dj_db:  7.121e-05   w:  6.861572e-03, b:-1.138181e-03
    Iteration 2000000: Cost 4.20e-07  dj_dw: -1.001e-06, dj_db:  1.851e-05   w:  6.882723e-03, b:-1.529307e-03
    Iteration 3000000: Cost 4.19e-07  dj_dw: -2.601e-07, dj_db:  4.810e-06   w:  6.888221e-03, b:-1.630965e-03
    Iteration 4000000: Cost 4.19e-07  dj_dw: -6.761e-08, dj_db:  1.250e-06   w:  6.889650e-03, b:-1.657387e-03
    Iteration 5000000: Cost 4.19e-07  dj_dw: -1.757e-08, dj_db:  3.250e-07   w:  6.890021e-03, b:-1.664255e-03
    Iteration 6000000: Cost 4.19e-07  dj_dw: -4.568e-09, dj_db:  8.446e-08   w:  6.890118e-03, b:-1.666040e-03
    Iteration 7000000: Cost 4.19e-07  dj_dw: -1.187e-09, dj_db:  2.195e-08   w:  6.890143e-03, b:-1.666504e-03
    Iteration 8000000: Cost 4.19e-07  dj_dw: -3.086e-10, dj_db:  5.706e-09   w:  6.890149e-03, b:-1.666624e-03
    Iteration 9000000: Cost 4.19e-07  dj_dw: -8.021e-11, dj_db:  1.483e-09   w:  6.890151e-03, b:-1.666656e-03
    (w,b) found by gradient descent: (0.00689015,-0.00166666)


我又把迭代次数加了100倍，步长相应加长了些，运行足足76s，截距和斜率都向R的结果靠近，斜率的值已经非常接近了。看来R和卡西欧用的应该不是这种算法，它们都是一瞬间完成的，还要准确的多。我下面用随机梯度下降法试试，应该会快一些。

### 随机梯度下降法

#### 单点SGD
每碰到一个点就计算一次梯度，有随机性但整体是收敛的，适用于大数据量情况，实际中更常用。这个小数据量内部大量随机取值应该也能达到效果。



```python
def sgd(w, b, x, y, alpha, iterations):
    m = x.shape[0]
    J_hist = []
    p_hist = []

    for t in range(iterations):
        # 随机选择一个样本
        random_index = np.random.randint(m)
        x_i = x[random_index]
        y_i = y[random_index]

        # 计算梯度
        dj_dw = 2 * x_i * (x_i * w + b - y_i)
        dj_db = 2 * (x_i * w + b - y_i)
        # 更新参数
        w -= alpha * dj_dw
        b -= alpha * dj_db
        
        if t<10000000:
            J_hist.append((w * x_i + b - y_i) ** 2 / (2))
            p_hist.append([w,b])
        # 间隔10%打印
        if t% math.ceil(iterations/10) == 0:
            print(f"Iteration {t:4}: Cost {J_hist[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.6e}, b:{b: 0.6e}")
                
    return w, b, J_hist, p_hist
```


```python
# w = np.random.randn(1, 1)[0][0] # 随机初始化参数
# b = np.random.randn(1, 1)[0][0] # 注意原输出为数组
w = 0
b = 0
iterations = 10000000
tmp_alpha = 1.0e-5
w_final, b_final, J_hist1, p_hist1 = sgd(w, b, x, y, tmp_alpha, iterations)
print(f"(w,b) found by gradient descent: ({w_final:8.8f},{b_final:8.8f})")
```

    Iteration    0: Cost 7.72e-04  dj_dw: -4.720e-01, dj_db: -7.867e-02   w:  4.720000e-06, b: 7.866667e-07
    Iteration 1000000: Cost 1.03e-06  dj_dw:  2.871e-02, dj_db:  2.871e-03   w:  6.879212e-03, b:-1.526250e-03
    Iteration 2000000: Cost 6.76e-08  dj_dw:  4.417e-03, dj_db:  7.361e-04   w:  6.892617e-03, b:-1.654578e-03
    Iteration 3000000: Cost 9.22e-09  dj_dw:  2.175e-03, dj_db:  2.719e-04   w:  6.891841e-03, b:-1.665608e-03
    Iteration 4000000: Cost 1.17e-08  dj_dw:  3.687e-03, dj_db:  3.073e-04   w:  6.887850e-03, b:-1.667678e-03
    Iteration 5000000: Cost 1.50e-08  dj_dw:  4.171e-03, dj_db:  3.475e-04   w:  6.889270e-03, b:-1.664641e-03
    Iteration 6000000: Cost 1.60e-06  dj_dw: -5.028e-02, dj_db: -3.591e-03   w:  6.896112e-03, b:-1.667515e-03
    Iteration 7000000: Cost 1.08e-06  dj_dw: -4.723e-02, dj_db: -2.952e-03   w:  6.887534e-03, b:-1.668866e-03
    Iteration 8000000: Cost 8.75e-08  dj_dw:  2.031e-02, dj_db:  8.463e-04   w:  6.892321e-03, b:-1.664104e-03
    Iteration 9000000: Cost 3.32e-07  dj_dw:  4.293e-02, dj_db:  1.651e-03   w:  6.890429e-03, b:-1.670188e-03
    (w,b) found by gradient descent: (0.00688775,-0.00166655)


随机梯度下降算法的优势显而易见。这里没办法用损失函数直接比较，因为本算法不需要把梯度求和再算损失函数，实际上算损失函数占据了主要算力。

但是我们可以将结果和R拟合的值进行比较，本算法同样迭代了一千万次，步长也一样，但是只用了21s，快了将近4倍。精确度方面不好比较，理论上参数合适的话应该是batch梯度下降更精确一些。



### 展望

我了解实际应用中还有小批量梯度下降，介于单点和整体梯度下降之间。可惜我的时间实在不够了暂时搁置。本次讨论至少知道R语言和卡西欧计算器线性拟合不可能用的传统的batch梯度下降。其实还有一种正态方程线性拟合的方法，数学上非常优美，但是普适性不强，这里就不做比较了
